import logging
import asyncio
from datetime import datetime, timedelta
from openai import AsyncOpenAI
from app.configuracion import config_energetico

logger = logging.getLogger(__name__)

class OpenRouterClient:
    def __init__(self):
        # üéØ CONFIGURACI√ìN PARA OPENROTER V√çA OPENAI
        self.client = AsyncOpenAI(
            api_key=config_energetico.IA_API_KEY,
            base_url="https://openrouter.ai/api/v1"  # ‚Üê URL espec√≠fica de OpenRouter
        )
        
        self.modelo_actual = config_energetico.IA_MODELO_DEFAULT
        self.max_tokens = config_energetico.IA_MAX_TOKENS
        self.timeout = config_energetico.IA_TIMEOUT
        self.temperature = 0.3
        
        # üõ°Ô∏è Rate limiting
        self.last_request_time = None
        self.min_request_interval = 1.5
        
        # üíæ Cache
        self._cache = {}
        self._cache_ttl = 300
        
        if not config_energetico.IA_API_KEY:
            raise ValueError("‚ùå OpenRouter API Key no configurada")
        
        logger.info(f"ü§ñ Cliente OpenRouter inicializado - Modelo: {self.modelo_actual}")
    
    async def _wait_rate_limit(self):
        """Esperar entre requests para evitar rate limiting"""
        if self.last_request_time:
            elapsed = (datetime.now() - self.last_request_time).total_seconds()
            if elapsed < self.min_request_interval:
                wait_time = self.min_request_interval - elapsed
                await asyncio.sleep(wait_time)
        self.last_request_time = datetime.now()
    
    def _get_cache_key(self, prompt: str, contexto: str) -> str:
        """Generar clave √∫nica para cache"""
        return f"{hash(prompt)}:{hash(contexto)}"
    
    def _get_cached_response(self, cache_key: str) -> str:
        """Obtener respuesta del cache si existe y es v√°lida"""
        if cache_key in self._cache:
            timestamp, response = self._cache[cache_key]
            if datetime.now() - timestamp < timedelta(seconds=self._cache_ttl):
                logger.info("üíæ Respuesta obtenida de cache")
                return response
            else:
                del self._cache[cache_key]
        return None
    
    def _set_cached_response(self, cache_key: str, response: str):
        """Guardar respuesta en cache"""
        if len(self._cache) >= 10:
            oldest_key = min(self._cache.keys(), key=lambda k: self._cache[k][0])
            del self._cache[oldest_key]
        self._cache[cache_key] = (datetime.now(), response)
    
    async def consultar_ia(self, prompt: str, contexto: str = "", modelo: str = None) -> str:
        """Consultar OpenRouter usando la librer√≠a OpenAI oficial"""
        
        modelo_usar = modelo or self.modelo_actual
        
        # Optimizar contexto y prompt
        contexto_optimizado = self._optimizar_contexto(contexto)
        prompt_optimizado = self._optimizar_prompt(prompt)
        
        cache_key = self._get_cache_key(prompt_optimizado, contexto_optimizado)
        cached_response = self._get_cached_response(cache_key)
        
        if cached_response:
            return cached_response
        
        try:
            # üõ°Ô∏è Rate limiting
            await self._wait_rate_limit()
            
            # üéØ SYSTEM MESSAGE optimizada
            system_message = """Eres un experto en eficiencia energ√©tica industrial 
            y an√°lisis de datos de consumo el√©ctrico en M√©xico. Especialista en 
            tarifas CFE GDMTH. Proporciona an√°lisis precisos y recomendaciones pr√°cticas."""
            
            # üìù Preparar mensajes para la API
            messages = [
                {"role": "system", "content": system_message}
            ]
            
            # A√±adir contexto si existe
            if contexto_optimizado:
                messages.append({"role": "user", "content": f"CONTEXTO:\n{contexto_optimizado}"})
            
            # A√±adir prompt principal
            messages.append({"role": "user", "content": f"PREGUNTA/AN√ÅLISIS:\n{prompt_optimizado}"})
            
            logger.info(f"üîç Consultando OpenRouter - Modelo: {modelo_usar}")
            
            # üöÄ LLAMADA A LA API CON LIBRER√çA OPENAI
            response = await self.client.chat.completions.create(
                model=modelo_usar,
                messages=messages,
                max_tokens=self.max_tokens,
                temperature=self.temperature,
                timeout=self.timeout
            )
            
            # üìä Procesar respuesta
            respuesta = response.choices[0].message.content
            
            # üíæ Guardar en cache
            self._set_cached_response(cache_key, respuesta)
            
            # üìà Log de uso
            tokens_usados = response.usage.total_tokens if response.usage else 0
            logger.info(f"‚úÖ OpenRouter ({modelo_usar}): {tokens_usados} tokens usados")
            
            return respuesta
            
        except Exception as e:
            logger.error(f"‚ùå Error en OpenRouter ({modelo_usar}): {str(e)}")
            
            # üîÑ Intentar fallback con otro modelo si es error de modelo espec√≠fico
            # Evitar recursi√≥n infinita: solo hacer fallback si NO estamos ya en un fallback
            if (modelo is None) and ("model" in str(e).lower() or "not found" in str(e).lower() or "404" in str(e)):
                return await self._fallback_modelo(prompt_optimizado, contexto_optimizado, modelo_usar)
            
            return f"‚ùå Error en consulta IA: {str(e)}"
    
    async def _fallback_modelo(self, prompt: str, contexto: str, modelo_fallido: str) -> str:
        """Fallback a otros modelos gratuitos si uno falla"""
        
        logger.warning(f"üîÑ Modelo '{modelo_fallido}' fall√≥. Iniciando fallback...")
        modelos_fallback = [
            "meta-llama/llama-3.3-8b-instruct:free", # Reemplazo de llama-3.1
            "google/gemma-3-4b-it:free",           # Reemplazo de gemini-flash-1.5
            "mistralai/mistral-7b-instruct:free",  # Sigue siendo v√°lido
            "z-ai/glm-4.5-air:free"                # Opci√≥n robusta adicional
        ]
        
        # Remover el modelo que fall√≥ (si estuviera en la lista, aunque no deber√≠a)
        modelos_fallback = [m for m in modelos_fallback if m != modelo_fallido]
        
        for modelo in modelos_fallback:
            try:
                # IMPORTANTE: Aqu√≠ llamamos a consultar_ia CON el modelo de fallback
                # para evitar que vuelva a entrar en esta funci√≥n de fallback
                respuesta = await self.consultar_ia(prompt, contexto, modelo)
                
                if respuesta and not respuesta.startswith("‚ùå"):
                    logger.info(f"‚úÖ Fallback exitoso con modelo: {modelo}")
                    return respuesta
                else:
                    logger.warning(f"‚ö†Ô∏è Fallback con {modelo} tambi√©n fall√≥ (respuesta inv√°lida)")

            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Fallback {modelo} tambi√©n fall√≥ con error: {e}")
                continue
        
        logger.error("‚ùå Todos los modelos de fallback fallaron.")
        return "‚ùå Todos los modelos (principal y de respaldo) est√°n temporalmente no disponibles."
    
    def _optimizar_contexto(self, contexto: str) -> str:
        """Optimizar contexto para reducir tokens"""
        if not contexto:
            return ""
        
        lines = contexto.strip().split('\n')
        lines_optimizadas = []
        
        for line in lines:
            line = ' '.join(line.split())
            if len(line) > 120:
                line = line[:117] + "..."
            if line and not line.isspace():
                lines_optimizadas.append(line)
        
        return '\n'.join(lines_optimizadas[:12])
    
    def _optimizar_prompt(self, prompt: str) -> str:
        """Optimizar prompt"""
        prompt = ' '.join(prompt.split())
        
        prompts_optimizados = {
            "analisis general": "Analiza patrones principales y da 3 recomendaciones clave con ahorro estimado para tarifa GDMTH M√©xico",
            "optimizacion costos": "Estrategias para reducir costos manteniendo operaciones en industria mexicana",
            "demanda maxima": "C√≥mo optimizar demanda m√°xima y reducir penalizaciones CFE en tarifa GDMTH",
            "factor potencia": "Recomendaciones para mejorar factor de potencia >90% en industria con tarifa GDMTH"
        }
        
        prompt_lower = prompt.lower()
        for key, optimized in prompts_optimizados.items():
            if key in prompt_lower:
                return optimized
        
        if len(prompt) > 180:
            return prompt[:177] + "..."
        
        return prompt
    
    async def analizar_datos_energeticos(self, df_summary: str, pregunta_especifica: str = "") -> str:
        """M√©todo especializado para an√°lisis energ√©tico"""
        
        # --- AQU√ç ESTABA EL ERROR ---
        # Antes: contexto = f"Datos energ√©ticos M√©xico GDMTH:\n{self._optimizar_contexo(df_summary)}"
        # Ahora:
        contexto = f"Datos energ√©ticos M√©xico GDMTH:\n{self._optimizar_contexto(df_summary)}"
        
        if pregunta_especifica:
            prompt = self._optimizar_prompt(pregunta_especifica)
        else:
            prompt = "Analiza patrones principales y da 3 recomendaciones clave con ahorro estimado para tarifa GDMTH M√©xico"
        
        # Usar√° el modelo principal (o fallback si falla)
        return await self.consultar_ia(prompt, contexto)
    
    def obtener_modelos_disponibles(self) -> list:
        """Obtener lista de modelos gratuitos disponibles"""
        # Esta funci√≥n parece venir de tu config, aseg√∫rate que est√© actualizada
        return config_energetico.obtener_modelos_disponibles()
    
    async def probar_conexion(self) -> bool:
        """Probar conexi√≥n con OpenRouter"""
        try:
            respuesta = await self.consultar_ia("Responde solo con 'OK' si est√°s funcionando.", "")
            return "OK" in respuesta.upper()
        except Exception as e:
            logger.error(f"‚ùå Prueba de conexi√≥n fall√≥: {e}")
            return False